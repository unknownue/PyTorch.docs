{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Plot randomly generated multilabel dataset\n\nThis illustrates the :func:`~sklearn.datasets.make_multilabel_classification`\ndataset generator. Each sample consists of counts of two features (up to 50 in\ntotal), which are differently distributed in each of two classes.\n\nPoints are labeled as follows, where Y means the class is present:\n\n    =====  =====  =====  ======\n      1      2      3    Color\n    =====  =====  =====  ======\n      Y      N      N    Red\n      N      Y      N    Blue\n      N      N      Y    Yellow\n      Y      Y      N    Purple\n      Y      N      Y    Orange\n      Y      Y      N    Green\n      Y      Y      Y    Brown\n    =====  =====  =====  ======\n\nA star marks the expected sample for each class; its size reflects the\nprobability of selecting that class label.\n\nThe left and right examples highlight the ``n_labels`` parameter:\nmore of the samples in the right plot have 2 or 3 labels.\n\nNote that this two-dimensional example is very degenerate:\ngenerally the number of features would be much greater than the\n\"document length\", while here we have much larger documents than vocabulary.\nSimilarly, with ``n_classes > n_features``, it is much less likely that a\nfeature distinguishes a particular class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import make_multilabel_classification as make_ml_clf\n\nCOLORS = np.array(\n    [\n        \"!\",\n        \"#FF3333\",  # red\n        \"#0198E1\",  # blue\n        \"#BF5FFF\",  # purple\n        \"#FCD116\",  # yellow\n        \"#FF7216\",  # orange\n        \"#4DBD33\",  # green\n        \"#87421F\",  # brown\n    ]\n)\n\n# Use same random seed for multiple calls to make_multilabel_classification to\n# ensure same distributions\nRANDOM_SEED = np.random.randint(2**10)\n\n\ndef plot_2d(ax, n_labels=1, n_classes=3, length=50):\n    X, Y, p_c, p_w_c = make_ml_clf(\n        n_samples=150,\n        n_features=2,\n        n_classes=n_classes,\n        n_labels=n_labels,\n        length=length,\n        allow_unlabeled=False,\n        return_distributions=True,\n        random_state=RANDOM_SEED,\n    )\n\n    ax.scatter(\n        X[:, 0], X[:, 1], color=COLORS.take((Y * [1, 2, 4]).sum(axis=1)), marker=\".\"\n    )\n    ax.scatter(\n        p_w_c[0] * length,\n        p_w_c[1] * length,\n        marker=\"*\",\n        linewidth=0.5,\n        edgecolor=\"black\",\n        s=20 + 1500 * p_c**2,\n        color=COLORS.take([1, 2, 4]),\n    )\n    ax.set_xlabel(\"Feature 0 count\")\n    return p_c, p_w_c\n\n\n_, (ax1, ax2) = plt.subplots(1, 2, sharex=\"row\", sharey=\"row\", figsize=(8, 4))\nplt.subplots_adjust(bottom=0.15)\n\np_c, p_w_c = plot_2d(ax1, n_labels=1)\nax1.set_title(\"n_labels=1, length=50\")\nax1.set_ylabel(\"Feature 1 count\")\n\nplot_2d(ax2, n_labels=3)\nax2.set_title(\"n_labels=3, length=50\")\nax2.set_xlim(left=0, auto=True)\nax2.set_ylim(bottom=0, auto=True)\n\nplt.show()\n\nprint(\"The data was generated from (random_state=%d):\" % RANDOM_SEED)\nprint(\"Class\", \"P(C)\", \"P(w0|C)\", \"P(w1|C)\", sep=\"\\t\")\nfor k, p, p_w in zip([\"red\", \"blue\", \"yellow\"], p_c, p_w_c.T):\n    print(\"%s\\t%0.2f\\t%0.2f\\t%0.2f\" % (k, p, p_w[0], p_w[1]))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}