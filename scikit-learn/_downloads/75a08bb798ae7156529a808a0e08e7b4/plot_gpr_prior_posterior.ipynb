{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Illustration of prior and posterior Gaussian process for different kernels\n\nThis example illustrates the prior and posterior of a GPR with different\nkernels. Mean, standard deviation, and 10 samples are shown for both prior\nand posterior.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(__doc__)\n\n# Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n#\n# License: BSD 3 clause\n\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import (RBF, Matern, RationalQuadratic,\n                                              ExpSineSquared, DotProduct,\n                                              ConstantKernel)\n\n\nkernels = [1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-1, 10.0)),\n           1.0 * RationalQuadratic(length_scale=1.0, alpha=0.1),\n           1.0 * ExpSineSquared(length_scale=1.0, periodicity=3.0,\n                                length_scale_bounds=(0.1, 10.0),\n                                periodicity_bounds=(1.0, 10.0)),\n           ConstantKernel(0.1, (0.01, 10.0))\n               * (DotProduct(sigma_0=1.0, sigma_0_bounds=(0.1, 10.0)) ** 2),\n           1.0 * Matern(length_scale=1.0, length_scale_bounds=(1e-1, 10.0),\n                        nu=1.5)]\n\nfor kernel in kernels:\n    # Specify Gaussian Process\n    gp = GaussianProcessRegressor(kernel=kernel)\n\n    # Plot prior\n    plt.figure(figsize=(8, 8))\n    plt.subplot(2, 1, 1)\n    X_ = np.linspace(0, 5, 100)\n    y_mean, y_std = gp.predict(X_[:, np.newaxis], return_std=True)\n    plt.plot(X_, y_mean, 'k', lw=3, zorder=9)\n    plt.fill_between(X_, y_mean - y_std, y_mean + y_std,\n                     alpha=0.2, color='k')\n    y_samples = gp.sample_y(X_[:, np.newaxis], 10)\n    plt.plot(X_, y_samples, lw=1)\n    plt.xlim(0, 5)\n    plt.ylim(-3, 3)\n    plt.title(\"Prior (kernel:  %s)\" % kernel, fontsize=12)\n\n    # Generate data and fit GP\n    rng = np.random.RandomState(4)\n    X = rng.uniform(0, 5, 10)[:, np.newaxis]\n    y = np.sin((X[:, 0] - 2.5) ** 2)\n    gp.fit(X, y)\n\n    # Plot posterior\n    plt.subplot(2, 1, 2)\n    X_ = np.linspace(0, 5, 100)\n    y_mean, y_std = gp.predict(X_[:, np.newaxis], return_std=True)\n    plt.plot(X_, y_mean, 'k', lw=3, zorder=9)\n    plt.fill_between(X_, y_mean - y_std, y_mean + y_std,\n                     alpha=0.2, color='k')\n\n    y_samples = gp.sample_y(X_[:, np.newaxis], 10)\n    plt.plot(X_, y_samples, lw=1)\n    plt.scatter(X[:, 0], y, c='r', s=50, zorder=10, edgecolors=(0, 0, 0))\n    plt.xlim(0, 5)\n    plt.ylim(-3, 3)\n    plt.title(\"Posterior (kernel: %s)\\n Log-Likelihood: %.3f\"\n              % (gp.kernel_, gp.log_marginal_likelihood(gp.kernel_.theta)),\n              fontsize=12)\n    plt.tight_layout()\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}