{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# SVM: Separating hyperplane for unbalanced classes\n\nFind the optimal separating hyperplane using an SVC for classes that\nare unbalanced.\n\nWe first find the separating plane with a plain SVC and then plot\n(dashed) the separating hyperplane with automatically correction for\nunbalanced classes.\n\n.. currentmodule:: sklearn.linear_model\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This example will also work by replacing ``SVC(kernel=\"linear\")``\n    with ``SGDClassifier(loss=\"hinge\")``. Setting the ``loss`` parameter\n    of the :class:`SGDClassifier` equal to ``hinge`` will yield behaviour\n    such as that of a SVC with a linear kernel.\n\n    For example try instead of the ``SVC``::\n\n        clf = SGDClassifier(n_iter=100, alpha=0.01)</p></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\nfrom sklearn.datasets import make_blobs\n\n# we create two clusters of random points\nn_samples_1 = 1000\nn_samples_2 = 100\ncenters = [[0.0, 0.0], [2.0, 2.0]]\nclusters_std = [1.5, 0.5]\nX, y = make_blobs(n_samples=[n_samples_1, n_samples_2],\n                  centers=centers,\n                  cluster_std=clusters_std,\n                  random_state=0, shuffle=False)\n\n# fit the model and get the separating hyperplane\nclf = svm.SVC(kernel='linear', C=1.0)\nclf.fit(X, y)\n\n# fit the model and get the separating hyperplane using weighted classes\nwclf = svm.SVC(kernel='linear', class_weight={1: 10})\nwclf.fit(X, y)\n\n# plot the samples\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors='k')\n\n# plot the decision functions for both classifiers\nax = plt.gca()\nxlim = ax.get_xlim()\nylim = ax.get_ylim()\n\n# create grid to evaluate model\nxx = np.linspace(xlim[0], xlim[1], 30)\nyy = np.linspace(ylim[0], ylim[1], 30)\nYY, XX = np.meshgrid(yy, xx)\nxy = np.vstack([XX.ravel(), YY.ravel()]).T\n\n# get the separating hyperplane\nZ = clf.decision_function(xy).reshape(XX.shape)\n\n# plot decision boundary and margins\na = ax.contour(XX, YY, Z, colors='k', levels=[0], alpha=0.5, linestyles=['-'])\n\n# get the separating hyperplane for weighted classes\nZ = wclf.decision_function(xy).reshape(XX.shape)\n\n# plot decision boundary and margins for weighted classes\nb = ax.contour(XX, YY, Z, colors='r', levels=[0], alpha=0.5, linestyles=['-'])\n\nplt.legend([a.collections[0], b.collections[0]], [\"non weighted\", \"weighted\"],\n           loc=\"upper right\")\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}