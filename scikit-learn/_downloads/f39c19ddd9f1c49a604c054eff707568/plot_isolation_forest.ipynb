{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# IsolationForest example\n\nAn example using :class:`~sklearn.ensemble.IsolationForest` for anomaly\ndetection.\n\nThe `isolation_forest` is an ensemble of \"Isolation Trees\" that \"isolate\"\nobservations by recursive random partitioning, which can be represented by a\ntree structure. The number of splittings required to isolate a sample is lower\nfor outliers and higher for inliers.\n\nIn the present example we demo two ways to visualize the decision boundary of an\nIsolation Forest trained on a toy dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data generation\n\nWe generate two clusters (each one containing `n_samples`) by randomly\nsampling the standard normal distribution as returned by\n:func:`numpy.random.randn`. One of them is spherical and the other one is\nslightly deformed.\n\nFor consistency with the :class:`~sklearn.ensemble.IsolationForest` notation,\nthe inliers (i.e. the gaussian clusters) are assigned a ground truth label `1`\nwhereas the outliers (created with :func:`numpy.random.uniform`) are assigned\nthe label `-1`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nfrom sklearn.model_selection import train_test_split\n\nn_samples, n_outliers = 120, 40\nrng = np.random.RandomState(0)\ncovariance = np.array([[0.5, -0.1], [0.7, 0.4]])\ncluster_1 = 0.4 * rng.randn(n_samples, 2) @ covariance + np.array([2, 2])  # general\ncluster_2 = 0.3 * rng.randn(n_samples, 2) + np.array([-2, -2])  # spherical\noutliers = rng.uniform(low=-4, high=4, size=(n_outliers, 2))\n\nX = np.concatenate([cluster_1, cluster_2, outliers])\ny = np.concatenate(\n    [np.ones((2 * n_samples), dtype=int), -np.ones((n_outliers), dtype=int)]\n)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can visualize the resulting clusters:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nscatter = plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\"k\")\nhandles, labels = scatter.legend_elements()\nplt.axis(\"square\")\nplt.legend(handles=handles, labels=[\"outliers\", \"inliers\"], title=\"true class\")\nplt.title(\"Gaussian inliers with \\nuniformly distributed outliers\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training of the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import IsolationForest\n\nclf = IsolationForest(max_samples=100, random_state=0)\nclf.fit(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot discrete decision boundary\n\nWe use the class :class:`~sklearn.inspection.DecisionBoundaryDisplay` to\nvisualize a discrete decision boundary. The background color represents\nwhether a sample in that given area is predicted to be an outlier\nor not. The scatter plot displays the true labels.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\ndisp = DecisionBoundaryDisplay.from_estimator(\n    clf,\n    X,\n    response_method=\"predict\",\n    alpha=0.5,\n)\ndisp.ax_.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\"k\")\ndisp.ax_.set_title(\"Binary decision boundary \\nof IsolationForest\")\nplt.axis(\"square\")\nplt.legend(handles=handles, labels=[\"outliers\", \"inliers\"], title=\"true class\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot path length decision boundary\n\nBy setting the `response_method=\"decision_function\"`, the background of the\n:class:`~sklearn.inspection.DecisionBoundaryDisplay` represents the measure of\nnormality of an observation. Such score is given by the path length averaged\nover a forest of random trees, which itself is given by the depth of the leaf\n(or equivalently the number of splits) required to isolate a given sample.\n\nWhen a forest of random trees collectively produce short path lengths for\nisolating some particular samples, they are highly likely to be anomalies and\nthe measure of normality is close to `0`. Similarly, large paths correspond to\nvalues close to `1` and are more likely to be inliers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "disp = DecisionBoundaryDisplay.from_estimator(\n    clf,\n    X,\n    response_method=\"decision_function\",\n    alpha=0.5,\n)\ndisp.ax_.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\"k\")\ndisp.ax_.set_title(\"Path length decision boundary \\nof IsolationForest\")\nplt.axis(\"square\")\nplt.legend(handles=handles, labels=[\"outliers\", \"inliers\"], title=\"true class\")\nplt.colorbar(disp.ax_.collections[1])\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}