{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# One-class SVM with non-linear kernel (RBF)\n\nAn example using a one-class SVM for novelty detection.\n\n`One-class SVM <svm_outlier_detection>` is an unsupervised\nalgorithm that learns a decision function for novelty detection:\nclassifying new data as similar or different to the training set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager\nfrom sklearn import svm\n\nxx, yy = np.meshgrid(np.linspace(-5, 5, 500), np.linspace(-5, 5, 500))\n# Generate train data\nX = 0.3 * np.random.randn(100, 2)\nX_train = np.r_[X + 2, X - 2]\n# Generate some regular novel observations\nX = 0.3 * np.random.randn(20, 2)\nX_test = np.r_[X + 2, X - 2]\n# Generate some abnormal novel observations\nX_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))\n\n# fit the model\nclf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)\nclf.fit(X_train)\ny_pred_train = clf.predict(X_train)\ny_pred_test = clf.predict(X_test)\ny_pred_outliers = clf.predict(X_outliers)\nn_error_train = y_pred_train[y_pred_train == -1].size\nn_error_test = y_pred_test[y_pred_test == -1].size\nn_error_outliers = y_pred_outliers[y_pred_outliers == 1].size\n\n# plot the line, the points, and the nearest vectors to the plane\nZ = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.title(\"Novelty Detection\")\nplt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.PuBu)\na = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='darkred')\nplt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='palevioletred')\n\ns = 40\nb1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white', s=s, edgecolors='k')\nb2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='blueviolet', s=s,\n                 edgecolors='k')\nc = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='gold', s=s,\n                edgecolors='k')\nplt.axis('tight')\nplt.xlim((-5, 5))\nplt.ylim((-5, 5))\nplt.legend([a.collections[0], b1, b2, c],\n           [\"learned frontier\", \"training observations\",\n            \"new regular observations\", \"new abnormal observations\"],\n           loc=\"upper left\",\n           prop=matplotlib.font_manager.FontProperties(size=11))\nplt.xlabel(\n    \"error train: %d/200 ; errors novel regular: %d/40 ; \"\n    \"errors novel abnormal: %d/40\"\n    % (n_error_train, n_error_test, n_error_outliers))\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}