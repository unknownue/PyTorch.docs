{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Imputing missing values with variants of IterativeImputer\n\n\nThe :class:`sklearn.impute.IterativeImputer` class is very flexible - it can be\nused with a variety of estimators to do round-robin regression, treating every\nvariable as an output in turn.\n\nIn this example we compare some estimators for the purpose of missing feature\nimputation with :class:`sklearn.impute.IterativeImputer`:\n\n* :class:`~sklearn.linear_model.BayesianRidge`: regularized linear regression\n* :class:`~sklearn.tree.DecisionTreeRegressor`: non-linear regression\n* :class:`~sklearn.ensemble.ExtraTreesRegressor`: similar to missForest in R\n* :class:`~sklearn.neighbors.KNeighborsRegressor`: comparable to other KNN\n  imputation approaches\n\nOf particular interest is the ability of\n:class:`sklearn.impute.IterativeImputer` to mimic the behavior of missForest, a\npopular imputation package for R. In this example, we have chosen to use\n:class:`sklearn.ensemble.ExtraTreesRegressor` instead of\n:class:`sklearn.ensemble.RandomForestRegressor` (as in missForest) due to its\nincreased speed.\n\nNote that :class:`sklearn.neighbors.KNeighborsRegressor` is different from KNN\nimputation, which learns from samples with missing values by using a distance\nmetric that accounts for missing values, rather than imputing them.\n\nThe goal is to compare different estimators to see which one is best for the\n:class:`sklearn.impute.IterativeImputer` when using a\n:class:`sklearn.linear_model.BayesianRidge` estimator on the California housing\ndataset with a single value randomly removed from each row.\n\nFor this particular pattern of missing values we see that\n:class:`sklearn.ensemble.ExtraTreesRegressor` and\n:class:`sklearn.linear_model.BayesianRidge` give the best results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# To use this experimental feature, we need to explicitly ask for it:\nfrom sklearn.experimental import enable_iterative_imputer  # noqa\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score\n\nN_SPLITS = 5\n\nrng = np.random.RandomState(0)\n\nX_full, y_full = fetch_california_housing(return_X_y=True)\n# ~2k samples is enough for the purpose of the example.\n# Remove the following two lines for a slower run with different error bars.\nX_full = X_full[::10]\ny_full = y_full[::10]\nn_samples, n_features = X_full.shape\n\n# Estimate the score on the entire dataset, with no missing values\nbr_estimator = BayesianRidge()\nscore_full_data = pd.DataFrame(\n    cross_val_score(\n        br_estimator, X_full, y_full, scoring='neg_mean_squared_error',\n        cv=N_SPLITS\n    ),\n    columns=['Full Data']\n)\n\n# Add a single missing value to each row\nX_missing = X_full.copy()\ny_missing = y_full\nmissing_samples = np.arange(n_samples)\nmissing_features = rng.choice(n_features, n_samples, replace=True)\nX_missing[missing_samples, missing_features] = np.nan\n\n# Estimate the score after imputation (mean and median strategies)\nscore_simple_imputer = pd.DataFrame()\nfor strategy in ('mean', 'median'):\n    estimator = make_pipeline(\n        SimpleImputer(missing_values=np.nan, strategy=strategy),\n        br_estimator\n    )\n    score_simple_imputer[strategy] = cross_val_score(\n        estimator, X_missing, y_missing, scoring='neg_mean_squared_error',\n        cv=N_SPLITS\n    )\n\n# Estimate the score after iterative imputation of the missing values\n# with different estimators\nestimators = [\n    BayesianRidge(),\n    DecisionTreeRegressor(max_features='sqrt', random_state=0),\n    ExtraTreesRegressor(n_estimators=10, random_state=0),\n    KNeighborsRegressor(n_neighbors=15)\n]\nscore_iterative_imputer = pd.DataFrame()\nfor impute_estimator in estimators:\n    estimator = make_pipeline(\n        IterativeImputer(random_state=0, estimator=impute_estimator),\n        br_estimator\n    )\n    score_iterative_imputer[impute_estimator.__class__.__name__] = \\\n        cross_val_score(\n            estimator, X_missing, y_missing, scoring='neg_mean_squared_error',\n            cv=N_SPLITS\n        )\n\nscores = pd.concat(\n    [score_full_data, score_simple_imputer, score_iterative_imputer],\n    keys=['Original', 'SimpleImputer', 'IterativeImputer'], axis=1\n)\n\n# plot boston results\nfig, ax = plt.subplots(figsize=(13, 6))\nmeans = -scores.mean()\nerrors = scores.std()\nmeans.plot.barh(xerr=errors, ax=ax)\nax.set_title('California Housing Regression with Different Imputation Methods')\nax.set_xlabel('MSE (smaller is better)')\nax.set_yticks(np.arange(means.shape[0]))\nax.set_yticklabels([\" w/ \".join(label) for label in means.index.get_values()])\nplt.tight_layout(pad=1)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}