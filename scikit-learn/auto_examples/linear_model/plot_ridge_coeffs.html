

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Ridge coefficients as a function of the L2 Regularization" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://scikit-learn/stable/auto_examples/linear_model/plot_ridge_coeffs.html" />
<meta property="og:site_name" content="scikit-learn" />
<meta property="og:description" content="A model that overfits learns the training data too well, capturing both the underlying patterns and the noise in the data. However, when applied to unseen data, the learned associations may not hol..." />
<meta property="og:image" content="https://scikit-learn.org/stable/_static/scikit-learn-logo-small.png" />
<meta property="og:image:alt" content="scikit-learn" />
<meta name="description" content="A model that overfits learns the training data too well, capturing both the underlying patterns and the noise in the data. However, when applied to unseen data, the learned associations may not hol..." />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  
  <title>Ridge coefficients as a function of the L2 Regularization &mdash; scikit-learn 1.3.2 documentation</title>
  
  <link rel="canonical" href="http://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_coeffs.html" />

  
  <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  

  <link rel="stylesheet" href="../../_static/css/vendor/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/plot_directive.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
<script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
<script src="../../_static/js/vendor/jquery-3.6.3.slim.min.js"></script> 
</head>
<body>






<nav id="navbar" class="sk-docs-navbar navbar navbar-expand-md navbar-light bg-light py-0">
  <div class="container-fluid sk-docs-container px-0">
      <a class="navbar-brand py-0" href="../../index.html">
        <img
          class="sk-brand-img"
          src="../../_static/scikit-learn-logo-small.png"
          alt="logo"/>
      </a>
    <button
      id="sk-navbar-toggler"
      class="navbar-toggler"
      type="button"
      data-toggle="collapse"
      data-target="#navbarSupportedContent"
      aria-controls="navbarSupportedContent"
      aria-expanded="false"
      aria-label="Toggle navigation"
    >
      <span class="navbar-toggler-icon"></span>
    </button>

    <div class="sk-navbar-collapse collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav mr-auto">
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../../install.html">Install</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../../user_guide.html">User Guide</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../../modules/classes.html">API</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../index.html">Examples</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" target="_blank" rel="noopener noreferrer" href="https://blog.scikit-learn.org/">Community</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../getting_started.html" >Getting Started</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../tutorial/index.html" >Tutorial</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../whats_new/v1.3.html" >What's new</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../glossary.html" >Glossary</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="https://scikit-learn.org/dev/developers/index.html" target="_blank" rel="noopener noreferrer">Development</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../faq.html" >FAQ</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../support.html" >Support</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../related_projects.html" >Related packages</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../roadmap.html" >Roadmap</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../governance.html" >Governance</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../about.html" >About us</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="https://github.com/scikit-learn/scikit-learn" >GitHub</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="https://scikit-learn.org/dev/versions.html" >Other Versions and Download</a>
        </li>
        <li class="nav-item dropdown nav-more-item-dropdown">
          <a class="sk-nav-link nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">More</a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdown">
              <a class="sk-nav-dropdown-item dropdown-item" href="../../getting_started.html" >Getting Started</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../../tutorial/index.html" >Tutorial</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../../whats_new/v1.3.html" >What's new</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../../glossary.html" >Glossary</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="https://scikit-learn.org/dev/developers/index.html" target="_blank" rel="noopener noreferrer">Development</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../../faq.html" >FAQ</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../../support.html" >Support</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../../related_projects.html" >Related packages</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../../roadmap.html" >Roadmap</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../../governance.html" >Governance</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../../about.html" >About us</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="https://github.com/scikit-learn/scikit-learn" >GitHub</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="https://scikit-learn.org/dev/versions.html" >Other Versions and Download</a>
          </div>
        </li>
      </ul>
      <div id="searchbox" role="search">
          <div class="searchformwrapper">
          <form class="search" action="../../search.html" method="get">
            <input class="sk-search-text-input" type="text" name="q" aria-labelledby="searchlabel" />
            <input class="sk-search-text-btn" type="submit" value="Go" />
          </form>
          </div>
      </div>
    </div>
  </div>
</nav>
<div class="d-flex" id="sk-doc-wrapper">
    <input type="checkbox" name="sk-toggle-checkbox" id="sk-toggle-checkbox">
    <label id="sk-sidemenu-toggle" class="sk-btn-toggle-toc btn sk-btn-primary" for="sk-toggle-checkbox">Toggle Menu</label>
    <div id="sk-sidebar-wrapper" class="border-right">
      <div class="sk-sidebar-toc-wrapper">
        <div class="btn-group w-100 mb-2" role="group" aria-label="rellinks">
            <a href="plot_logistic_path.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="Regularization path of L1- Logistic Regression">Prev</a><a href="index.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="Generalized Linear Models">Up</a>
            <a href="plot_robust_fit.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="Robust linear estimator fitting">Next</a>
        </div>
        <div class="alert alert-danger p-1 mb-2" role="alert">
          <p class="text-center mb-0">
          <strong>scikit-learn 1.3.2</strong><br/>
          <a href="http://scikit-learn.org/dev/versions.html">Other versions</a>
          </p>
        </div>
        <div class="alert alert-warning p-1 mb-2" role="alert">
          <p class="text-center mb-0">
            Please <a class="font-weight-bold" href="../../about.html#citing-scikit-learn"><string>cite us</string></a> if you use the software.
          </p>
        </div>
            <div class="sk-sidebar-toc">
              <ul>
<li><a class="reference internal" href="#">Ridge coefficients as a function of the L2 Regularization</a><ul>
<li><a class="reference internal" href="#purpose-of-this-example">Purpose of this example</a><ul>
<li><a class="reference internal" href="#creating-a-non-noisy-data-set">Creating a non-noisy data set</a></li>
<li><a class="reference internal" href="#training-the-ridge-regressor">Training the Ridge Regressor</a></li>
<li><a class="reference internal" href="#plotting-trained-coefficients-and-mean-squared-errors">Plotting trained Coefficients and Mean Squared Errors</a></li>
<li><a class="reference internal" href="#interpreting-the-plots">Interpreting the plots</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
      </div>
    </div>
    <div id="sk-page-content-wrapper">
      <div class="sk-page-content container-fluid body px-md-3" role="main">
        
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-auto-examples-linear-model-plot-ridge-coeffs-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code or to run this example in your browser via Binder</p>
</div>
<section class="sphx-glr-example-title" id="ridge-coefficients-as-a-function-of-the-l2-regularization">
<span id="sphx-glr-auto-examples-linear-model-plot-ridge-coeffs-py"></span><h1>Ridge coefficients as a function of the L2 Regularization<a class="headerlink" href="#ridge-coefficients-as-a-function-of-the-l2-regularization" title="Link to this heading">¶</a></h1>
<p>A model that overfits learns the training data too well, capturing both the
underlying patterns and the noise in the data. However, when applied to unseen
data, the learned associations may not hold. We normally detect this when we
apply our trained predictions to the test data and see the statistical
performance drop significantly compared to the training data.</p>
<p>One way to overcome overfitting is through regularization, which can be done by
penalizing large weights (coefficients) in linear models, forcing the model to
shrink all coefficients. Regularization reduces a model’s reliance on specific
information obtained from the training samples.</p>
<p>This example illustrates how L2 regularization in a
<a class="reference internal" href="../../modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code class="xref py py-class docutils literal notranslate"><span class="pre">Ridge</span></code></a> regression affects a model’s performance by
adding a penalty term to the loss that increases with the coefficients
<span class="math notranslate nohighlight">\(\beta\)</span>.</p>
<p>The regularized loss function is given by: <span class="math notranslate nohighlight">\(\mathcal{L}(X, y, \beta) =
\| y - X \beta \|^{2}_{2} + \alpha \| \beta \|^{2}_{2}\)</span></p>
<p>where <span class="math notranslate nohighlight">\(X\)</span> is the input data, <span class="math notranslate nohighlight">\(y\)</span> is the target variable,
<span class="math notranslate nohighlight">\(\beta\)</span> is the vector of coefficients associated with the features, and
<span class="math notranslate nohighlight">\(\alpha\)</span> is the regularization strength.</p>
<p>The regularized loss function aims to balance the trade-off between accurately
predicting the training set and to prevent overfitting.</p>
<p>In this regularized loss, the left-hand side (e.g. <span class="math notranslate nohighlight">\(\|y -
X\beta\|^{2}_{2}\)</span>) measures the squared difference between the actual target
variable, <span class="math notranslate nohighlight">\(y\)</span>, and the predicted values. Minimizing this term alone could
lead to overfitting, as the model may become too complex and sensitive to noise
in the training data.</p>
<p>To address overfitting, Ridge regularization adds a constraint, called a penalty
term, (<span class="math notranslate nohighlight">\(\alpha \| \beta\|^{2}_{2}\)</span>) to the loss function. This penalty
term is the sum of the squares of the model’s coefficients, multiplied by the
regularization strength <span class="math notranslate nohighlight">\(\alpha\)</span>. By introducing this constraint, Ridge
regularization discourages any single coefficient <span class="math notranslate nohighlight">\(\beta_{i}\)</span> from taking
an excessively large value and encourages smaller and more evenly distributed
coefficients. Higher values of <span class="math notranslate nohighlight">\(\alpha\)</span> force the coefficients towards
zero. However, an excessively high <span class="math notranslate nohighlight">\(\alpha\)</span> can result in an underfit
model that fails to capture important patterns in the data.</p>
<p>Therefore, the regularized loss function combines the prediction accuracy term
and the penalty term. By adjusting the regularization strength, practitioners
can fine-tune the degree of constraint imposed on the weights, training a model
capable of generalizing well to unseen data while avoiding overfitting.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Author: Kornel Kielczewski -- &lt;kornel.k@plusnet.pl&gt;</span>
</pre></div>
</div>
<section id="purpose-of-this-example">
<h2>Purpose of this example<a class="headerlink" href="#purpose-of-this-example" title="Link to this heading">¶</a></h2>
<p>For the purpose of showing how Ridge regularization works, we will create a
non-noisy data set. Then we will train a regularized model on a range of
regularization strengths (<span class="math notranslate nohighlight">\(\alpha\)</span>) and plot how the trained
coefficients and the mean squared error between those and the original values
behave as functions of the regularization strength.</p>
<section id="creating-a-non-noisy-data-set">
<h3>Creating a non-noisy data set<a class="headerlink" href="#creating-a-non-noisy-data-set" title="Link to this heading">¶</a></h3>
<p>We make a toy data set with 100 samples and 10 features, that’s suitable to
detect regression. Out of the 10 features, 8 are informative and contribute to
the regression, while the remaining 2 features do not have any effect on the
target variable (their true coefficients are 0). Please note that in this
example the data is non-noisy, hence we can expect our regression model to
recover exactly the true coefficients w.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <a href="../../modules/generated/sklearn.datasets.make_regression.html#sklearn.datasets.make_regression" title="sklearn.datasets.make_regression" class="sphx-glr-backref-module-sklearn-datasets sphx-glr-backref-type-py-function"><span class="n">make_regression</span></a>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <a href="../../modules/generated/sklearn.datasets.make_regression.html#sklearn.datasets.make_regression" title="sklearn.datasets.make_regression" class="sphx-glr-backref-module-sklearn-datasets sphx-glr-backref-type-py-function"><span class="n">make_regression</span></a><span class="p">(</span>
    <span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">coef</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span>
<span class="p">)</span>

<span class="c1"># Obtain the true coefficients</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The true coefficient of this regression problem are:</span><span class="se">\n</span><span class="si">{</span><span class="n">w</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>The true coefficient of this regression problem are:
[38.32634568 88.49665188  0.         29.75747153  0.         19.08699432
 25.44381023 38.69892343 49.28808734 71.75949622]
</pre></div>
</div>
</section>
<section id="training-the-ridge-regressor">
<h3>Training the Ridge Regressor<a class="headerlink" href="#training-the-ridge-regressor" title="Link to this heading">¶</a></h3>
<p>We use <a class="reference internal" href="../../modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code class="xref py py-class docutils literal notranslate"><span class="pre">Ridge</span></code></a>, a linear model with L2
regularization. We train several models, each with a different value for the
model parameter <code class="docutils literal notranslate"><span class="pre">alpha</span></code>, which is a positive constant that multiplies the
penalty term, controlling the regularization strength. For each trained model
we then compute the error between the true coefficients <code class="docutils literal notranslate"><span class="pre">w</span></code> and the
coefficients found by the model <code class="docutils literal notranslate"><span class="pre">clf</span></code>. We store the identified coefficients
and the calculated errors for the corresponding coefficients in lists, which
makes it convenient for us to plot them.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <a href="../../modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge" class="sphx-glr-backref-module-sklearn-linear_model sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">Ridge</span></a>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <a href="../../modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error" title="sklearn.metrics.mean_squared_error" class="sphx-glr-backref-module-sklearn-metrics sphx-glr-backref-type-py-function"><span class="n">mean_squared_error</span></a>

<span class="n">clf</span> <span class="o">=</span> <a href="../../modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge" class="sphx-glr-backref-module-sklearn-linear_model sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">Ridge</span></a><span class="p">()</span>

<span class="c1"># Generate values for `alpha` that are evenly distributed on a logarithmic scale</span>
<span class="n">alphas</span> <span class="o">=</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.logspace.html#numpy.logspace" title="numpy.logspace" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-function"><span class="n">np</span><span class="o">.</span><span class="n">logspace</span></a><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">coefs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">errors_coefs</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Train the model with different regularisation strengths</span>
<span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">:</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">coefs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
    <span class="n">errors_coefs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><a href="../../modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error" title="sklearn.metrics.mean_squared_error" class="sphx-glr-backref-module-sklearn-metrics sphx-glr-backref-type-py-function"><span class="n">mean_squared_error</span></a><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">w</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="plotting-trained-coefficients-and-mean-squared-errors">
<h3>Plotting trained Coefficients and Mean Squared Errors<a class="headerlink" href="#plotting-trained-coefficients-and-mean-squared-errors" title="Link to this heading">¶</a></h3>
<p>We now plot the 10 different regularized coefficients as a function of the
regularization parameter <code class="docutils literal notranslate"><span class="pre">alpha</span></code> where each color represents a different
coefficient.</p>
<p>On the right-hand-side, we plot how the errors of the coefficients from the
estimator change as a function of regularization.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">alphas</span> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.html#pandas.Index" title="pandas.Index" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">pd</span><span class="o">.</span><span class="n">Index</span></a><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;alpha&quot;</span><span class="p">)</span>
<span class="n">coefs</span> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.DataFrame" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span></a><span class="p">(</span><span class="n">coefs</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">alphas</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;Feature </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)])</span>
<span class="n">errors</span> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.Series" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">pd</span><span class="o">.</span><span class="n">Series</span></a><span class="p">(</span><span class="n">errors_coefs</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">alphas</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Mean squared error&quot;</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html#matplotlib.pyplot.subplots" title="matplotlib.pyplot.subplots" class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span></a><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">coefs</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">logx</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Ridge coefficients as a function of the regularization strength&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Ridge coefficient values&quot;</span><span class="p">)</span>
<span class="n">errors</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="n">logx</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Coefficient error as a function of the regularization strength&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Mean squared error&quot;</span><span class="p">)</span>
</pre></div>
</div>
<img src="../../_images/sphx_glr_plot_ridge_coeffs_001.png" srcset="../../_images/sphx_glr_plot_ridge_coeffs_001.png" alt="Ridge coefficients as a function of the regularization strength, Coefficient error as a function of the regularization strength" class = "sphx-glr-single-img"/></section>
<section id="interpreting-the-plots">
<h3>Interpreting the plots<a class="headerlink" href="#interpreting-the-plots" title="Link to this heading">¶</a></h3>
<p>The plot on the left-hand side shows how the regularization strength (<code class="docutils literal notranslate"><span class="pre">alpha</span></code>)
affects the Ridge regression coefficients. Smaller values of <code class="docutils literal notranslate"><span class="pre">alpha</span></code> (weak
regularization), allow the coefficients to closely resemble the true
coefficients (<code class="docutils literal notranslate"><span class="pre">w</span></code>) used to generate the data set. This is because no
additional noise was added to our artificial data set. As <code class="docutils literal notranslate"><span class="pre">alpha</span></code> increases,
the coefficients shrink towards zero, gradually reducing the impact of the
features that were formerly more significant.</p>
<p>The right-hand side plot shows the mean squared error (MSE) between the
coefficients found by the model and the true coefficients (<code class="docutils literal notranslate"><span class="pre">w</span></code>). It provides a
measure that relates to how exact our ridge model is in comparison to the true
generative model. A low error means that it found coefficients closer to the
ones of the true generative model. In this case, since our toy data set was
non-noisy, we can see that the least regularized model retrieves coefficients
closest to the true coefficients (<code class="docutils literal notranslate"><span class="pre">w</span></code>) (error is close to 0).</p>
<p>When <code class="docutils literal notranslate"><span class="pre">alpha</span></code> is small, the model captures the intricate details of the
training data, whether those were caused by noise or by actual information. As
<code class="docutils literal notranslate"><span class="pre">alpha</span></code> increases, the highest coefficients shrink more rapidly, rendering
their corresponding features less influential in the training process. This
can enhance a model’s ability to generalize to unseen data (if there was a lot
of noise to capture), but it also poses the risk of losing performance if the
regularization becomes too strong compared to the amount of noise the data
contained (as in this example).</p>
<p>In real-world scenarios where data typically includes noise, selecting an
appropriate <code class="docutils literal notranslate"><span class="pre">alpha</span></code> value becomes crucial in striking a balance between an
overfitting and an underfitting model.</p>
<p>Here, we saw that <a class="reference internal" href="../../modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code class="xref py py-class docutils literal notranslate"><span class="pre">Ridge</span></code></a> adds a penalty to the
coefficients to fight overfitting. Another problem that occurs is linked to
the presence of outliers in the training dataset. An outlier is a data point
that differs significantly from other observations. Concretely, these outliers
impact the left-hand side term of the loss function that we showed earlier.
Some other linear models are formulated to be robust to outliers such as the
<a class="reference internal" href="../../modules/generated/sklearn.linear_model.HuberRegressor.html#sklearn.linear_model.HuberRegressor" title="sklearn.linear_model.HuberRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">HuberRegressor</span></code></a>. You can learn more about it in
the <a class="reference internal" href="plot_huber_vs_ridge.html#sphx-glr-auto-examples-linear-model-plot-huber-vs-ridge-py"><span class="std std-ref">HuberRegressor vs Ridge on dataset with strong outliers</span></a> example.</p>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 0.431 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-auto-examples-linear-model-plot-ridge-coeffs-py">
<div class="binder-badge docutils container">
<a class="reference external image-reference" href="https://mybinder.org/v2/gh/scikit-learn/scikit-learn/1.3.X?urlpath=lab/tree/notebooks/auto_examples/linear_model/plot_ridge_coeffs.ipynb"><img alt="Launch binder" src="../../_images/binder_badge_logo18.svg" width="150px" /></a>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/b075b4610b24c70d5757248abef3fb61/plot_ridge_coeffs.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">plot_ridge_coeffs.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/44775ca43ce935c840a877c3d2ebed6c/plot_ridge_coeffs.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">plot_ridge_coeffs.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>
</section>


      </div>
    <div class="container">
      <footer class="sk-content-footer">
            &copy; 2007 - 2023, scikit-learn developers (BSD License).
          <a href="../../_sources/auto_examples/linear_model/plot_ridge_coeffs.rst.txt" rel="nofollow">Show this page source</a>
      </footer>
    </div>
  </div>
</div>
<script src="../../_static/js/vendor/bootstrap.min.js"></script>

<script>
    window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
    ga('create', 'UA-22606712-2', 'auto');
    ga('set', 'anonymizeIp', true);
    ga('send', 'pageview');
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>



<script defer data-domain="scikit-learn.org" src="https://views.scientific-python.org/js/script.js">
</script>


<script src="../../_static/clipboard.min.js"></script>
<script src="../../_static/copybutton.js"></script>

<script>
$(document).ready(function() {
    /* Add a [>>>] button on the top-right corner of code samples to hide
     * the >>> and ... prompts and the output and thus make the code
     * copyable. */
    var div = $('.highlight-python .highlight,' +
                '.highlight-python3 .highlight,' +
                '.highlight-pycon .highlight,' +
		'.highlight-default .highlight')
    var pre = div.find('pre');

    // get the styles from the current theme
    pre.parent().parent().css('position', 'relative');

    // create and add the button to all the code blocks that contain >>>
    div.each(function(index) {
        var jthis = $(this);
        // tracebacks (.gt) contain bare text elements that need to be
        // wrapped in a span to work with .nextUntil() (see later)
        jthis.find('pre:has(.gt)').contents().filter(function() {
            return ((this.nodeType == 3) && (this.data.trim().length > 0));
        }).wrap('<span>');
    });

	/*** Add permalink buttons next to glossary terms ***/
	$('dl.glossary > dt[id]').append(function() {
		return ('<a class="headerlink" href="#' +
			    this.getAttribute('id') +
			    '" title="Permalink to this term">¶</a>');
	});
});

</script>
    
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
    
</body>
</html>