

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="Description" content="scikit-learn: machine learning in Python">

  
  <title>sklearn.linear_model.SGDRegressor &mdash; scikit-learn 0.23.2 documentation</title>
  
  <link rel="canonical" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html" />

  
  <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  

  <link rel="stylesheet" href="../../_static/css/vendor/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
<script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
<script src="../../_static/jquery.js"></script> 
</head>
<body>
<nav id="navbar" class="sk-docs-navbar navbar navbar-expand-md navbar-light bg-light py-0">
  <div class="container-fluid sk-docs-container px-0">
      <a class="navbar-brand py-0" href="../../index.html">
        <img
          class="sk-brand-img"
          src="../../_static/scikit-learn-logo-small.png"
          alt="logo"/>
      </a>
    <button
      id="sk-navbar-toggler"
      class="navbar-toggler"
      type="button"
      data-toggle="collapse"
      data-target="#navbarSupportedContent"
      aria-controls="navbarSupportedContent"
      aria-expanded="false"
      aria-label="Toggle navigation"
    >
      <span class="navbar-toggler-icon"></span>
    </button>

    <div class="sk-navbar-collapse collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav mr-auto">
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../../install.html">Install</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../../user_guide.html">User Guide</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../classes.html">API</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../../auto_examples/index.html">Examples</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../getting_started.html">Getting Started</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../tutorial/index.html">Tutorial</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../whats_new/v0.23.html">What's new</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../glossary.html">Glossary</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../developers/index.html">Development</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../faq.html">FAQ</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../related_projects.html">Related packages</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../roadmap.html">Roadmap</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../about.html">About us</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="https://github.com/scikit-learn/scikit-learn">GitHub</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="https://scikit-learn.org/dev/versions.html">Other Versions</a>
        </li>
        <li class="nav-item dropdown nav-more-item-dropdown">
          <a class="sk-nav-link nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">More</a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdown">
              <a class="sk-nav-dropdown-item dropdown-item" href="../../getting_started.html">Getting Started</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../../tutorial/index.html">Tutorial</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../../whats_new/v0.23.html">What's new</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../../glossary.html">Glossary</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../../developers/index.html">Development</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../../faq.html">FAQ</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../../related_projects.html">Related packages</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../../roadmap.html">Roadmap</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../../about.html">About us</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="https://github.com/scikit-learn/scikit-learn">GitHub</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="https://scikit-learn.org/dev/versions.html">Other Versions</a>
          </div>
        </li>
      </ul>
      <div id="searchbox" role="search">
          <div class="searchformwrapper">
          <form class="search" action="../../search.html" method="get">
            <input class="sk-search-text-input" type="text" name="q" aria-labelledby="searchlabel" />
            <input class="sk-search-text-btn" type="submit" value="Go" />
          </form>
          </div>
      </div>
    </div>
  </div>
</nav>
<div class="d-flex" id="sk-doc-wrapper">
    <input type="checkbox" name="sk-toggle-checkbox" id="sk-toggle-checkbox">
    <label id="sk-sidemenu-toggle" class="sk-btn-toggle-toc btn sk-btn-primary" for="sk-toggle-checkbox">Toggle Menu</label>
    <div id="sk-sidebar-wrapper" class="border-right">
      <div class="sk-sidebar-toc-wrapper">
        <div class="sk-sidebar-toc-logo">
          <a href="../../index.html">
            <img
              class="sk-brand-img"
              src="../../_static/scikit-learn-logo-small.png"
              alt="logo"/>
          </a>
        </div>
        <div class="btn-group w-100 mb-2" role="group" aria-label="rellinks">
            <a href="sklearn.linear_model.Ridge.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="sklearn.linear_model.Ridge">Prev</a><a href="../classes.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="API Reference">Up</a>
            <a href="sklearn.linear_model.ElasticNet.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="sklearn.linear_model.ElasticNet">Next</a>
        </div>
        <div class="alert alert-danger p-1 mb-2" role="alert">
          <p class="text-center mb-0">
          <strong>scikit-learn 0.23.2</strong><br/>
          <a href="http://scikit-learn.org/dev/versions.html">Other versions</a>
          </p>
        </div>
        <div class="alert alert-warning p-1 mb-2" role="alert">
          <p class="text-center mb-0">
            Please <a class="font-weight-bold" href="../../about.html#citing-scikit-learn"><string>cite us</string></a> if you use the software.
          </p>
        </div>
          <div class="sk-sidebar-toc">
            <ul>
<li><a class="reference internal" href="#"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.linear_model</span></code>.SGDRegressor</a><ul>
<li><a class="reference internal" href="#examples-using-sklearn-linear-model-sgdregressor">Examples using <code class="docutils literal notranslate"><span class="pre">sklearn.linear_model.SGDRegressor</span></code></a></li>
</ul>
</li>
</ul>

          </div>
      </div>
    </div>
    <div id="sk-page-content-wrapper">
      <div class="sk-page-content container-fluid body px-md-3" role="main">
        
  <div class="section" id="sklearn-linear-model-sgdregressor">
<h1><a class="reference internal" href="../classes.html#module-sklearn.linear_model" title="sklearn.linear_model"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.linear_model</span></code></a>.SGDRegressor<a class="headerlink" href="#sklearn-linear-model-sgdregressor" title="Permalink to this headline">¶</a></h1>
<dl class="py class">
<dt id="sklearn.linear_model.SGDRegressor">
<em class="property">class </em><code class="sig-prename descclassname">sklearn.linear_model.</code><code class="sig-name descname">SGDRegressor</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/linear_model/_stochastic_gradient.py#L1354"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sklearn.linear_model.SGDRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Linear model fitted by minimizing a regularized empirical loss with SGD</p>
<p>SGD stands for Stochastic Gradient Descent: the gradient of the loss is
estimated each sample at a time and the model is updated along the way with
a decreasing strength schedule (aka learning rate).</p>
<p>The regularizer is a penalty added to the loss function that shrinks model
parameters towards the zero vector using either the squared euclidean norm
L2 or the absolute norm L1 or a combination of both (Elastic Net). If the
parameter update crosses the 0.0 value because of the regularizer, the
update is truncated to 0.0 to allow for learning sparse models and achieve
online feature selection.</p>
<p>This implementation works with data represented as dense numpy arrays of
floating point values for the features.</p>
<p>Read more in the <a class="reference internal" href="../sgd.html#sgd"><span class="std std-ref">User Guide</span></a>.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>loss</strong><span class="classifier">str, default=’squared_loss’</span></dt><dd><p>The loss function to be used. The possible values are ‘squared_loss’,
‘huber’, ‘epsilon_insensitive’, or ‘squared_epsilon_insensitive’</p>
<p>The ‘squared_loss’ refers to the ordinary least squares fit.
‘huber’ modifies ‘squared_loss’ to focus less on getting outliers
correct by switching from squared to linear loss past a distance of
epsilon. ‘epsilon_insensitive’ ignores errors less than epsilon and is
linear past that; this is the loss function used in SVR.
‘squared_epsilon_insensitive’ is the same but becomes squared loss past
a tolerance of epsilon.</p>
<p>More details about the losses formulas can be found in the
<a class="reference internal" href="../sgd.html#sgd-mathematical-formulation"><span class="std std-ref">User Guide</span></a>.</p>
</dd>
<dt><strong>penalty</strong><span class="classifier">{‘l2’, ‘l1’, ‘elasticnet’}, default=’l2’</span></dt><dd><p>The penalty (aka regularization term) to be used. Defaults to ‘l2’
which is the standard regularizer for linear SVM models. ‘l1’ and
‘elasticnet’ might bring sparsity to the model (feature selection)
not achievable with ‘l2’.</p>
</dd>
<dt><strong>alpha</strong><span class="classifier">float, default=0.0001</span></dt><dd><p>Constant that multiplies the regularization term. The higher the
value, the stronger the regularization.
Also used to compute the learning rate when set to <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code> is
set to ‘optimal’.</p>
</dd>
<dt><strong>l1_ratio</strong><span class="classifier">float, default=0.15</span></dt><dd><p>The Elastic Net mixing parameter, with 0 &lt;= l1_ratio &lt;= 1.
l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.
Only used if <code class="docutils literal notranslate"><span class="pre">penalty</span></code> is ‘elasticnet’.</p>
</dd>
<dt><strong>fit_intercept</strong><span class="classifier">bool, default=True</span></dt><dd><p>Whether the intercept should be estimated or not. If False, the
data is assumed to be already centered.</p>
</dd>
<dt><strong>max_iter</strong><span class="classifier">int, default=1000</span></dt><dd><p>The maximum number of passes over the training data (aka epochs).
It only impacts the behavior in the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method, and not the
<a class="reference internal" href="#sklearn.linear_model.SGDRegressor.partial_fit" title="sklearn.linear_model.SGDRegressor.partial_fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">partial_fit</span></code></a> method.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</dd>
<dt><strong>tol</strong><span class="classifier">float, default=1e-3</span></dt><dd><p>The stopping criterion. If it is not None, training will stop
when (loss &gt; best_loss - tol) for <code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> consecutive
epochs.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</dd>
<dt><strong>shuffle</strong><span class="classifier">bool, default=True</span></dt><dd><p>Whether or not the training data should be shuffled after each epoch.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int, default=0</span></dt><dd><p>The verbosity level.</p>
</dd>
<dt><strong>epsilon</strong><span class="classifier">float, default=0.1</span></dt><dd><p>Epsilon in the epsilon-insensitive loss functions; only if <code class="docutils literal notranslate"><span class="pre">loss</span></code> is
‘huber’, ‘epsilon_insensitive’, or ‘squared_epsilon_insensitive’.
For ‘huber’, determines the threshold at which it becomes less
important to get the prediction exactly right.
For epsilon-insensitive, any differences between the current prediction
and the correct label are ignored if they are less than this threshold.</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int, RandomState instance, default=None</span></dt><dd><p>Used for shuffling the data, when <code class="docutils literal notranslate"><span class="pre">shuffle</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>.
Pass an int for reproducible output across multiple function calls.
See <a class="reference internal" href="../../glossary.html#term-random_state"><span class="xref std std-term">Glossary</span></a>.</p>
</dd>
<dt><strong>learning_rate</strong><span class="classifier">string, default=’invscaling’</span></dt><dd><p>The learning rate schedule:</p>
<ul>
<li><p>‘constant’: <code class="docutils literal notranslate"><span class="pre">eta</span> <span class="pre">=</span> <span class="pre">eta0</span></code></p></li>
<li><p>‘optimal’: <code class="docutils literal notranslate"><span class="pre">eta</span> <span class="pre">=</span> <span class="pre">1.0</span> <span class="pre">/</span> <span class="pre">(alpha</span> <span class="pre">*</span> <span class="pre">(t</span> <span class="pre">+</span> <span class="pre">t0))</span></code>
where t0 is chosen by a heuristic proposed by Leon Bottou.</p></li>
<li><p>‘invscaling’: <code class="docutils literal notranslate"><span class="pre">eta</span> <span class="pre">=</span> <span class="pre">eta0</span> <span class="pre">/</span> <span class="pre">pow(t,</span> <span class="pre">power_t)</span></code></p></li>
<li><p>‘adaptive’: eta = eta0, as long as the training keeps decreasing.
Each time n_iter_no_change consecutive epochs fail to decrease the
training loss by tol or fail to increase validation score by tol if
early_stopping is True, the current learning rate is divided by 5.</p>
<blockquote>
<div><div class="versionadded">
<p><span class="versionmodified added">New in version 0.20: </span>Added ‘adaptive’ option</p>
</div>
</div></blockquote>
</li>
</ul>
</dd>
<dt><strong>eta0</strong><span class="classifier">double, default=0.01</span></dt><dd><p>The initial learning rate for the ‘constant’, ‘invscaling’ or
‘adaptive’ schedules. The default value is 0.01.</p>
</dd>
<dt><strong>power_t</strong><span class="classifier">double, default=0.25</span></dt><dd><p>The exponent for inverse scaling learning rate.</p>
</dd>
<dt><strong>early_stopping</strong><span class="classifier">bool, default=False</span></dt><dd><p>Whether to use early stopping to terminate training when validation
score is not improving. If set to True, it will automatically set aside
a fraction of training data as validation and terminate
training when validation score returned by the <code class="docutils literal notranslate"><span class="pre">score</span></code> method is not
improving by at least <code class="docutils literal notranslate"><span class="pre">tol</span></code> for <code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> consecutive
epochs.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20: </span>Added ‘early_stopping’ option</p>
</div>
</dd>
<dt><strong>validation_fraction</strong><span class="classifier">float, default=0.1</span></dt><dd><p>The proportion of training data to set aside as validation set for
early stopping. Must be between 0 and 1.
Only used if <code class="docutils literal notranslate"><span class="pre">early_stopping</span></code> is True.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20: </span>Added ‘validation_fraction’ option</p>
</div>
</dd>
<dt><strong>n_iter_no_change</strong><span class="classifier">int, default=5</span></dt><dd><p>Number of iterations with no improvement to wait before early stopping.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20: </span>Added ‘n_iter_no_change’ option</p>
</div>
</dd>
<dt><strong>warm_start</strong><span class="classifier">bool, default=False</span></dt><dd><p>When set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.
See <a class="reference internal" href="../../glossary.html#term-warm_start"><span class="xref std std-term">the Glossary</span></a>.</p>
<p>Repeatedly calling fit or partial_fit when warm_start is True can
result in a different solution than when calling fit a single time
because of the way the data is shuffled.
If a dynamic learning rate is used, the learning rate is adapted
depending on the number of samples already seen. Calling <code class="docutils literal notranslate"><span class="pre">fit</span></code> resets
this counter, while <code class="docutils literal notranslate"><span class="pre">partial_fit</span></code>  will result in increasing the
existing counter.</p>
</dd>
<dt><strong>average</strong><span class="classifier">bool or int, default=False</span></dt><dd><p>When set to True, computes the averaged SGD weights accross all
updates and stores the result in the <code class="docutils literal notranslate"><span class="pre">coef_</span></code> attribute. If set to
an int greater than 1, averaging will begin once the total number of
samples seen reaches <code class="docutils literal notranslate"><span class="pre">average</span></code>. So <code class="docutils literal notranslate"><span class="pre">average=10</span></code> will begin
averaging after seeing 10 samples.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Attributes</dt>
<dd class="field-even"><dl>
<dt><strong>coef_</strong><span class="classifier">ndarray of shape (n_features,)</span></dt><dd><p>Weights assigned to the features.</p>
</dd>
<dt><strong>intercept_</strong><span class="classifier">ndarray of shape (1,)</span></dt><dd><p>The intercept term.</p>
</dd>
<dt><strong>average_coef_</strong><span class="classifier">ndarray of shape (n_features,)</span></dt><dd><p>Averaged weights assigned to the features. Only available
if <code class="docutils literal notranslate"><span class="pre">average=True</span></code>.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.23: </span>Attribute <code class="docutils literal notranslate"><span class="pre">average_coef_</span></code> was deprecated
in version 0.23 and will be removed in 0.25.</p>
</div>
</dd>
<dt><strong>average_intercept_</strong><span class="classifier">ndarray of shape (1,)</span></dt><dd><p>The averaged intercept term. Only available if <code class="docutils literal notranslate"><span class="pre">average=True</span></code>.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.23: </span>Attribute <code class="docutils literal notranslate"><span class="pre">average_intercept_</span></code> was deprecated
in version 0.23 and will be removed in 0.25.</p>
</div>
</dd>
<dt><strong>n_iter_</strong><span class="classifier">int</span></dt><dd><p>The actual number of iterations before reaching the stopping criterion.</p>
</dd>
<dt><strong>t_</strong><span class="classifier">int</span></dt><dd><p>Number of weight updates performed during training.
Same as <code class="docutils literal notranslate"><span class="pre">(n_iter_</span> <span class="pre">*</span> <span class="pre">n_samples)</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Ridge</span></code></a>, <a class="reference internal" href="sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet" title="sklearn.linear_model.ElasticNet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ElasticNet</span></code></a>, <a class="reference internal" href="sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso" title="sklearn.linear_model.Lasso"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Lasso</span></code></a>, <a class="reference internal" href="sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.svm.SVR</span></code></a></dt><dd></dd>
</dl>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Always scale the input. The most convenient way is to use a pipeline.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span>
<span class="gp">... </span>                    <span class="n">SGDRegressor</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">Pipeline(steps=[(&#39;standardscaler&#39;, StandardScaler()),</span>
<span class="go">                (&#39;sgdregressor&#39;, SGDRegressor())])</span>
</pre></div>
</div>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#sklearn.linear_model.SGDRegressor.densify" title="sklearn.linear_model.SGDRegressor.densify"><code class="xref py py-obj docutils literal notranslate"><span class="pre">densify</span></code></a>()</p></td>
<td><p>Convert coefficient matrix to dense array format.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sklearn.linear_model.SGDRegressor.fit" title="sklearn.linear_model.SGDRegressor.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(X, y[, coef_init, intercept_init, …])</p></td>
<td><p>Fit linear model with Stochastic Gradient Descent.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sklearn.linear_model.SGDRegressor.get_params" title="sklearn.linear_model.SGDRegressor.get_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_params</span></code></a>([deep])</p></td>
<td><p>Get parameters for this estimator.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sklearn.linear_model.SGDRegressor.partial_fit" title="sklearn.linear_model.SGDRegressor.partial_fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">partial_fit</span></code></a>(X, y[, sample_weight])</p></td>
<td><p>Perform one epoch of stochastic gradient descent on given samples.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sklearn.linear_model.SGDRegressor.predict" title="sklearn.linear_model.SGDRegressor.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(X)</p></td>
<td><p>Predict using the linear model</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sklearn.linear_model.SGDRegressor.score" title="sklearn.linear_model.SGDRegressor.score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">score</span></code></a>(X, y[, sample_weight])</p></td>
<td><p>Return the coefficient of determination R^2 of the prediction.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sklearn.linear_model.SGDRegressor.set_params" title="sklearn.linear_model.SGDRegressor.set_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_params</span></code></a>(**kwargs)</p></td>
<td><p>Set and validate the parameters of estimator.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sklearn.linear_model.SGDRegressor.sparsify" title="sklearn.linear_model.SGDRegressor.sparsify"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sparsify</span></code></a>()</p></td>
<td><p>Convert coefficient matrix to sparse format.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="sklearn.linear_model.SGDRegressor.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">loss</span><span class="o">=</span><span class="default_value">'squared_loss'</span></em>, <em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">penalty</span><span class="o">=</span><span class="default_value">'l2'</span></em>, <em class="sig-param"><span class="n">alpha</span><span class="o">=</span><span class="default_value">0.0001</span></em>, <em class="sig-param"><span class="n">l1_ratio</span><span class="o">=</span><span class="default_value">0.15</span></em>, <em class="sig-param"><span class="n">fit_intercept</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">max_iter</span><span class="o">=</span><span class="default_value">1000</span></em>, <em class="sig-param"><span class="n">tol</span><span class="o">=</span><span class="default_value">0.001</span></em>, <em class="sig-param"><span class="n">shuffle</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">epsilon</span><span class="o">=</span><span class="default_value">0.1</span></em>, <em class="sig-param"><span class="n">random_state</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">learning_rate</span><span class="o">=</span><span class="default_value">'invscaling'</span></em>, <em class="sig-param"><span class="n">eta0</span><span class="o">=</span><span class="default_value">0.01</span></em>, <em class="sig-param"><span class="n">power_t</span><span class="o">=</span><span class="default_value">0.25</span></em>, <em class="sig-param"><span class="n">early_stopping</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">validation_fraction</span><span class="o">=</span><span class="default_value">0.1</span></em>, <em class="sig-param"><span class="n">n_iter_no_change</span><span class="o">=</span><span class="default_value">5</span></em>, <em class="sig-param"><span class="n">warm_start</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">average</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/linear_model/_stochastic_gradient.py#L1563"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sklearn.linear_model.SGDRegressor.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize self.  See help(type(self)) for accurate signature.</p>
</dd></dl>

<dl class="py method">
<dt id="sklearn.linear_model.SGDRegressor.densify">
<code class="sig-name descname">densify</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/linear_model/_base.py#L337"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sklearn.linear_model.SGDRegressor.densify" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert coefficient matrix to dense array format.</p>
<p>Converts the <code class="docutils literal notranslate"><span class="pre">coef_</span></code> member (back) to a numpy.ndarray. This is the
default format of <code class="docutils literal notranslate"><span class="pre">coef_</span></code> and is required for fitting, so calling
this method is only required on models that have previously been
sparsified; otherwise, it is a no-op.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><dl class="simple">
<dt>self</dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sklearn.linear_model.SGDRegressor.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">coef_init</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">intercept_init</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">sample_weight</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/linear_model/_stochastic_gradient.py#L1215"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sklearn.linear_model.SGDRegressor.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit linear model with Stochastic Gradient Descent.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">{array-like, sparse matrix}, shape (n_samples, n_features)</span></dt><dd><p>Training data</p>
</dd>
<dt><strong>y</strong><span class="classifier">ndarray of shape (n_samples,)</span></dt><dd><p>Target values</p>
</dd>
<dt><strong>coef_init</strong><span class="classifier">ndarray of shape (n_features,), default=None</span></dt><dd><p>The initial coefficients to warm-start the optimization.</p>
</dd>
<dt><strong>intercept_init</strong><span class="classifier">ndarray of shape (1,), default=None</span></dt><dd><p>The initial intercept to warm-start the optimization.</p>
</dd>
<dt><strong>sample_weight</strong><span class="classifier">array-like, shape (n_samples,), default=None</span></dt><dd><p>Weights applied to individual samples (1. for unweighted).</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>self</strong><span class="classifier">returns an instance of self.</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sklearn.linear_model.SGDRegressor.get_params">
<code class="sig-name descname">get_params</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">deep</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/base.py#L189"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sklearn.linear_model.SGDRegressor.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>deep</strong><span class="classifier">bool, default=True</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>params</strong><span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sklearn.linear_model.SGDRegressor.partial_fit">
<code class="sig-name descname">partial_fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">sample_weight</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/linear_model/_stochastic_gradient.py#L1156"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sklearn.linear_model.SGDRegressor.partial_fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform one epoch of stochastic gradient descent on given samples.</p>
<p>Internally, this method uses <code class="docutils literal notranslate"><span class="pre">max_iter</span> <span class="pre">=</span> <span class="pre">1</span></code>. Therefore, it is not
guaranteed that a minimum of the cost function is reached after calling
it once. Matters such as objective convergence and early stopping
should be handled by the user.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">{array-like, sparse matrix}, shape (n_samples, n_features)</span></dt><dd><p>Subset of training data</p>
</dd>
<dt><strong>y</strong><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>Subset of target values</p>
</dd>
<dt><strong>sample_weight</strong><span class="classifier">array-like, shape (n_samples,), default=None</span></dt><dd><p>Weights applied to individual samples.
If not provided, uniform weights are assumed.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>self</strong><span class="classifier">returns an instance of self.</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sklearn.linear_model.SGDRegressor.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/linear_model/_stochastic_gradient.py#L1266"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sklearn.linear_model.SGDRegressor.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict using the linear model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">{array-like, sparse matrix}, shape (n_samples, n_features)</span></dt><dd></dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>ndarray of shape (n_samples,)</dt><dd><p>Predicted target values per element in X.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sklearn.linear_model.SGDRegressor.score">
<code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">sample_weight</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/base.py#L509"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sklearn.linear_model.SGDRegressor.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the coefficient of determination R^2 of the prediction.</p>
<p>The coefficient R^2 is defined as (1 - u/v), where u is the residual
sum of squares ((y_true - y_pred) ** 2).sum() and v is the total
sum of squares ((y_true - y_true.mean()) ** 2).sum().
The best possible score is 1.0 and it can be negative (because the
model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features,
would get a R^2 score of 0.0.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like of shape (n_samples, n_features)</span></dt><dd><p>Test samples. For some estimators this may be a
precomputed kernel matrix or a list of generic objects instead,
shape = (n_samples, n_samples_fitted),
where n_samples_fitted is the number of
samples used in the fitting for the estimator.</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like of shape (n_samples,) or (n_samples, n_outputs)</span></dt><dd><p>True values for X.</p>
</dd>
<dt><strong>sample_weight</strong><span class="classifier">array-like of shape (n_samples,), default=None</span></dt><dd><p>Sample weights.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>score</strong><span class="classifier">float</span></dt><dd><p>R^2 of self.predict(X) wrt. y.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The R2 score used when calling <code class="docutils literal notranslate"><span class="pre">score</span></code> on a regressor uses
<code class="docutils literal notranslate"><span class="pre">multioutput='uniform_average'</span></code> from version 0.23 to keep consistent
with default value of <a class="reference internal" href="sklearn.metrics.r2_score.html#sklearn.metrics.r2_score" title="sklearn.metrics.r2_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">r2_score</span></code></a>.
This influences the <code class="docutils literal notranslate"><span class="pre">score</span></code> method of all the multioutput
regressors (except for
<a class="reference internal" href="sklearn.multioutput.MultiOutputRegressor.html#sklearn.multioutput.MultiOutputRegressor" title="sklearn.multioutput.MultiOutputRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultiOutputRegressor</span></code></a>).</p>
</dd></dl>

<dl class="py method">
<dt id="sklearn.linear_model.SGDRegressor.set_params">
<code class="sig-name descname">set_params</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/linear_model/_stochastic_gradient.py#L103"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sklearn.linear_model.SGDRegressor.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set and validate the parameters of estimator.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>**kwargs</strong><span class="classifier">dict</span></dt><dd><p>Estimator parameters.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>self</strong><span class="classifier">object</span></dt><dd><p>Estimator instance.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sklearn.linear_model.SGDRegressor.sparsify">
<code class="sig-name descname">sparsify</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/linear_model/_base.py#L357"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sklearn.linear_model.SGDRegressor.sparsify" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert coefficient matrix to sparse format.</p>
<p>Converts the <code class="docutils literal notranslate"><span class="pre">coef_</span></code> member to a scipy.sparse matrix, which for
L1-regularized models can be much more memory- and storage-efficient
than the usual numpy.ndarray representation.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">intercept_</span></code> member is not converted.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><dl class="simple">
<dt>self</dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>For non-sparse models, i.e. when there are not many zeros in <code class="docutils literal notranslate"><span class="pre">coef_</span></code>,
this may actually <em>increase</em> memory usage, so use this method with
care. A rule of thumb is that the number of zero elements, which can
be computed with <code class="docutils literal notranslate"><span class="pre">(coef_</span> <span class="pre">==</span> <span class="pre">0).sum()</span></code>, must be more than 50% for this
to provide significant benefits.</p>
<p>After calling this method, further fitting with the partial_fit
method (if any) will not work until you call densify.</p>
</dd></dl>

</dd></dl>

<div class="section" id="examples-using-sklearn-linear-model-sgdregressor">
<h2>Examples using <code class="docutils literal notranslate"><span class="pre">sklearn.linear_model.SGDRegressor</span></code><a class="headerlink" href="#examples-using-sklearn-linear-model-sgdregressor" title="Permalink to this headline">¶</a></h2>
<div class="sphx-glr-thumbcontainer" tooltip="This is an example showing the prediction latency of various scikit-learn estimators."><div class="figure align-default" id="id1">
<img alt="Prediction Latency" src="../../_images/sphx_glr_plot_prediction_latency_thumb.png" />
<p class="caption"><span class="caption-text"><a class="reference internal" href="../../auto_examples/applications/plot_prediction_latency.html#sphx-glr-auto-examples-applications-plot-prediction-latency-py"><span class="std std-ref">Prediction Latency</span></a></span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Contours of where the penalty is equal to 1 for the three penalties L1, L2 and elastic-net."><div class="figure align-default" id="id2">
<img alt="SGD: Penalties" src="../../_images/sphx_glr_plot_sgd_penalties_thumb.png" />
<p class="caption"><span class="caption-text"><a class="reference internal" href="../../auto_examples/linear_model/plot_sgd_penalties.html#sphx-glr-auto-examples-linear-model-plot-sgd-penalties-py"><span class="std std-ref">SGD: Penalties</span></a></span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</div>
</div><div class="clearer"></div></div>
</div>


      </div>
    <div class="container">
      <footer class="sk-content-footer">
            &copy; 2007 - 2020, scikit-learn developers (BSD License).
          <a href="../../_sources/modules/generated/sklearn.linear_model.SGDRegressor.rst.txt" rel="nofollow">Show this page source</a>
      </footer>
    </div>
  </div>
</div>
<script src="../../_static/js/vendor/bootstrap.min.js"></script>

<script>
    window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
    ga('create', 'UA-22606712-2', 'auto');
    ga('set', 'anonymizeIp', true);
    ga('send', 'pageview');
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


<script>
$(document).ready(function() {
    /* Add a [>>>] button on the top-right corner of code samples to hide
     * the >>> and ... prompts and the output and thus make the code
     * copyable. */
    var div = $('.highlight-python .highlight,' +
                '.highlight-python3 .highlight,' +
                '.highlight-pycon .highlight,' +
		'.highlight-default .highlight')
    var pre = div.find('pre');

    // get the styles from the current theme
    pre.parent().parent().css('position', 'relative');
    var hide_text = 'Hide prompts and outputs';
    var show_text = 'Show prompts and outputs';

    // create and add the button to all the code blocks that contain >>>
    div.each(function(index) {
        var jthis = $(this);
        if (jthis.find('.gp').length > 0) {
            var button = $('<span class="copybutton">&gt;&gt;&gt;</span>');
            button.attr('title', hide_text);
            button.data('hidden', 'false');
            jthis.prepend(button);
        }
        // tracebacks (.gt) contain bare text elements that need to be
        // wrapped in a span to work with .nextUntil() (see later)
        jthis.find('pre:has(.gt)').contents().filter(function() {
            return ((this.nodeType == 3) && (this.data.trim().length > 0));
        }).wrap('<span>');
    });

    // define the behavior of the button when it's clicked
    $('.copybutton').click(function(e){
        e.preventDefault();
        var button = $(this);
        if (button.data('hidden') === 'false') {
            // hide the code output
            button.parent().find('.go, .gp, .gt').hide();
            button.next('pre').find('.gt').nextUntil('.gp, .go').css('visibility', 'hidden');
            button.css('text-decoration', 'line-through');
            button.attr('title', show_text);
            button.data('hidden', 'true');
        } else {
            // show the code output
            button.parent().find('.go, .gp, .gt').show();
            button.next('pre').find('.gt').nextUntil('.gp, .go').css('visibility', 'visible');
            button.css('text-decoration', 'none');
            button.attr('title', hide_text);
            button.data('hidden', 'false');
        }
    });

	/*** Add permalink buttons next to glossary terms ***/
	$('dl.glossary > dt[id]').append(function() {
		return ('<a class="headerlink" href="#' +
			    this.getAttribute('id') +
			    '" title="Permalink to this term">¶</a>');
	});
  /*** Hide navbar when scrolling down ***/
  // Returns true when headerlink target matches hash in url
  (function() {
    hashTargetOnTop = function() {
        var hash = window.location.hash;
        if ( hash.length < 2 ) { return false; }

        var target = document.getElementById( hash.slice(1) );
        if ( target === null ) { return false; }

        var top = target.getBoundingClientRect().top;
        return (top < 2) && (top > -2);
    };

    // Hide navbar on load if hash target is on top
    var navBar = document.getElementById("navbar");
    var navBarToggler = document.getElementById("sk-navbar-toggler");
    var navBarHeightHidden = "-" + navBar.getBoundingClientRect().height + "px";
    var $window = $(window);

    hideNavBar = function() {
        navBar.style.top = navBarHeightHidden;
    };

    showNavBar = function() {
        navBar.style.top = "0";
    }

    if (hashTargetOnTop()) {
        hideNavBar()
    }

    var prevScrollpos = window.pageYOffset;
    hideOnScroll = function(lastScrollTop) {
        if (($window.width() < 768) && (navBarToggler.getAttribute("aria-expanded") === 'true')) {
            return;
        }
        if (lastScrollTop > 2 && (prevScrollpos <= lastScrollTop) || hashTargetOnTop()){
            hideNavBar()
        } else {
            showNavBar()
        }
        prevScrollpos = lastScrollTop;
    };

    /*** high performance scroll event listener***/
    var raf = window.requestAnimationFrame ||
        window.webkitRequestAnimationFrame ||
        window.mozRequestAnimationFrame ||
        window.msRequestAnimationFrame ||
        window.oRequestAnimationFrame;
    var lastScrollTop = $window.scrollTop();

    if (raf) {
        loop();
    }

    function loop() {
        var scrollTop = $window.scrollTop();
        if (lastScrollTop === scrollTop) {
            raf(loop);
            return;
        } else {
            lastScrollTop = scrollTop;
            hideOnScroll(lastScrollTop);
            raf(loop);
        }
    }
  })();
});

</script>
    
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
    
</body>
</html>