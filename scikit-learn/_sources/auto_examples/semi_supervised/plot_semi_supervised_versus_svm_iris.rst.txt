
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/semi_supervised/plot_semi_supervised_versus_svm_iris.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_semi_supervised_plot_semi_supervised_versus_svm_iris.py>`
        to download the full example code or to run this example in your browser via Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_semi_supervised_plot_semi_supervised_versus_svm_iris.py:


===============================================================================
Decision boundary of semi-supervised classifiers versus SVM on the Iris dataset
===============================================================================

A comparison for the decision boundaries generated on the iris dataset
by Label Spreading, Self-training and SVM.

This example demonstrates that Label Spreading and Self-training can learn
good boundaries even when small amounts of labeled data are available.

Note that Self-training with 100% of the data is omitted as it is functionally
identical to training the SVC on 100% of the data.

.. GENERATED FROM PYTHON SOURCE LINES 16-90



.. image-sg:: /auto_examples/semi_supervised/images/sphx_glr_plot_semi_supervised_versus_svm_iris_001.png
   :alt: Unlabeled points are colored white, Label Spreading 30% data, Self-training 30% data, Label Spreading 50% data, Self-training 50% data, Label Spreading 100% data, SVC with rbf kernel
   :srcset: /auto_examples/semi_supervised/images/sphx_glr_plot_semi_supervised_versus_svm_iris_001.png
   :class: sphx-glr-single-img





.. code-block:: default


    # Authors: Clay Woolam   <clay@woolam.org>
    #          Oliver Rausch <rauscho@ethz.ch>
    # License: BSD

    import matplotlib.pyplot as plt
    import numpy as np

    from sklearn import datasets
    from sklearn.semi_supervised import LabelSpreading, SelfTrainingClassifier
    from sklearn.svm import SVC

    iris = datasets.load_iris()

    X = iris.data[:, :2]
    y = iris.target

    # step size in the mesh
    h = 0.02

    rng = np.random.RandomState(0)
    y_rand = rng.rand(y.shape[0])
    y_30 = np.copy(y)
    y_30[y_rand < 0.3] = -1  # set random samples to be unlabeled
    y_50 = np.copy(y)
    y_50[y_rand < 0.5] = -1
    # we create an instance of SVM and fit out data. We do not scale our
    # data since we want to plot the support vectors
    ls30 = (LabelSpreading().fit(X, y_30), y_30, "Label Spreading 30% data")
    ls50 = (LabelSpreading().fit(X, y_50), y_50, "Label Spreading 50% data")
    ls100 = (LabelSpreading().fit(X, y), y, "Label Spreading 100% data")

    # the base classifier for self-training is identical to the SVC
    base_classifier = SVC(kernel="rbf", gamma=0.5, probability=True)
    st30 = (
        SelfTrainingClassifier(base_classifier).fit(X, y_30),
        y_30,
        "Self-training 30% data",
    )
    st50 = (
        SelfTrainingClassifier(base_classifier).fit(X, y_50),
        y_50,
        "Self-training 50% data",
    )

    rbf_svc = (SVC(kernel="rbf", gamma=0.5).fit(X, y), y, "SVC with rbf kernel")

    # create a mesh to plot in
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

    color_map = {-1: (1, 1, 1), 0: (0, 0, 0.9), 1: (1, 0, 0), 2: (0.8, 0.6, 0)}

    classifiers = (ls30, st30, ls50, st50, ls100, rbf_svc)
    for i, (clf, y_train, title) in enumerate(classifiers):
        # Plot the decision boundary. For that, we will assign a color to each
        # point in the mesh [x_min, x_max]x[y_min, y_max].
        plt.subplot(3, 2, i + 1)
        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

        # Put the result into a color plot
        Z = Z.reshape(xx.shape)
        plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)
        plt.axis("off")

        # Plot also the training points
        colors = [color_map[y] for y in y_train]
        plt.scatter(X[:, 0], X[:, 1], c=colors, edgecolors="black")

        plt.title(title)

    plt.suptitle("Unlabeled points are colored white", y=0.1)
    plt.show()


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 0.967 seconds)


.. _sphx_glr_download_auto_examples_semi_supervised_plot_semi_supervised_versus_svm_iris.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: binder-badge

      .. image:: images/binder_badge_logo.svg
        :target: https://mybinder.org/v2/gh/scikit-learn/scikit-learn/1.3.X?urlpath=lab/tree/notebooks/auto_examples/semi_supervised/plot_semi_supervised_versus_svm_iris.ipynb
        :alt: Launch binder
        :width: 150 px



    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_semi_supervised_versus_svm_iris.py <plot_semi_supervised_versus_svm_iris.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_semi_supervised_versus_svm_iris.ipynb <plot_semi_supervised_versus_svm_iris.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
